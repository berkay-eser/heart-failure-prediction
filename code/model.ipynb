{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split ,KFold, cross_val_score, GridSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score\n",
    "# Classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv')\n",
    "X_test = pd.read_csv('../data/X_test.csv')\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "y_test = pd.read_csv('../data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Building\n",
    "\n",
    "Before model building and selection we have to decide that what is our goal? Which metric is more important for us? For example:\n",
    "\n",
    "- If false positives (incorrectly diagnosing a healthy person with heart disease) have serious consequences, we might prioritize precision.\n",
    "- If missing actual cases of heart disease is more concerning, we might prioritize recall.\n",
    "\n",
    "In this case, our priority will be recall but we also check other metrics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating KFold Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create KFold object to be sure that we have the same splits of the data every time. We pass kf object to cv parameter\n",
    "kf = KFold(n_splits=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Average Recall Scores of Different Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 Building Models for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "xgb = XGBClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "models= [lg, rf, xgb, dt, knc, svc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 KFold Cross-Validation for each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.837, 0.861, 0.825, 0.775, 0.835, 0.853]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_recall_scores = []\n",
    "\n",
    "for model in models:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=kf, scoring='recall')\n",
    "    avg_score = round(score.mean(),3)\n",
    "    avg_recall_scores.append(avg_score)\n",
    "\n",
    "avg_recall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier have best average recall score. So we proceed with RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Recall scores are: [0.91666667 0.77142857 0.92592593 0.86206897 0.86363636 0.85714286\n",
      " 0.74193548 0.75       0.79310345 0.92857143]\n",
      "Average Cross Validation Recall score:  0.84\n"
     ]
    }
   ],
   "source": [
    "rf_before = RandomForestClassifier()\n",
    "\n",
    "before_score = cross_val_score(rf_before, X_train, y_train, cv=kf, scoring='recall')\n",
    "before_avg_score = round(before_score.mean(),2)\n",
    "\n",
    "print('Cross Validation Recall scores are: {}'.format(before_score))\n",
    "print('Average Cross Validation Recall score: ', before_avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [130,150,170,190],\n",
    "    'max_depth': [8,10,12],\n",
    "    'min_samples_split': [3,4,5],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'random_state': [13]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid=rf_params, cv=kf, \n",
    "                          scoring='recall').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 150, 'random_state': 13}\n",
      "Best score: 0.8722376527604558\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_rf.best_params_)\n",
    "print('Best score:', grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 K-Fold Cross-Validation After Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Recall scores are: [0.91666667 0.82857143 0.92592593 0.93103448 0.86363636 0.85714286\n",
      " 0.77419355 0.83333333 0.82758621 0.96428571]\n",
      "Average Cross Validation Recall score:  0.872\n"
     ]
    }
   ],
   "source": [
    "rf_after = RandomForestClassifier(max_depth=10, min_samples_leaf=2,\n",
    "                                  min_samples_split=5, n_estimators=150, random_state=13)\n",
    "\n",
    "after_score = cross_val_score(rf_after, X_train, y_train, cv=kf, scoring='recall')\n",
    "after_avg_score = round(after_score.mean(), 3)\n",
    "\n",
    "print('Cross Validation Recall scores are: {}'.format(after_score))\n",
    "print('Average Cross Validation Recall score: ', after_avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Comparing Before-After Avg Recall Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall Score Before Hyperparameter Tuning: 0.84\n",
      "Average Recall Score After Hyperparameter Tuning: 0.872\n"
     ]
    }
   ],
   "source": [
    "print(f'Average Recall Score Before Hyperparameter Tuning: {before_avg_score}')\n",
    "print(f'Average Recall Score After Hyperparameter Tuning: {after_avg_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Metrics of Tuned Random Forest Classifier Model\n",
      "Accuracy: 0.907\n",
      "Recall: 0.886\n",
      "Precision: 0.933\n",
      "F1 Score: 0.909\n",
      "ROC-AUC: 0.908\n"
     ]
    }
   ],
   "source": [
    "rf_model = rf_after.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_recall = round(recall_score(y_test,y_pred),3)\n",
    "rf_accuracy = round(accuracy_score(y_test, y_pred),3)\n",
    "rf_precision = round(precision_score(y_test, y_pred),3)\n",
    "rf_f1 = round(f1_score(y_test, y_pred),3)\n",
    "rf_roc_auc =round(roc_auc_score(y_test, y_pred),3)\n",
    "\n",
    "print('All Metrics of Tuned Random Forest Classifier Model')\n",
    "print(f'Accuracy: {rf_accuracy}')\n",
    "print(f'Recall: {rf_recall}')\n",
    "print(f'Precision: {rf_precision}')\n",
    "print(f'F1 Score: {rf_f1}')\n",
    "print(f'ROC-AUC: {rf_roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
